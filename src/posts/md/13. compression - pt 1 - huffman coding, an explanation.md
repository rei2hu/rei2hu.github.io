# compression - pt 1 - Huffman coding, an explanation

This probably isn't the best title, but I was wondering if it was possible that
breaking data into multiple pieces and compressing them seperately can give
better results than compressing it at once.

I've actually changed the title like 4 times because the other ones were too long
for the layout.

## a basic compression technique - Huffman coding

Huffman coding is probably one of the simplest and well known compression techniques
considering it is taught as part of the computer science curriculum. I'll be rambling
about this in an effort to see whether this idea might be possible, but first, an
explanation of how it works.

### Huffman coding - an explanation

The idea behind Huffman coding is to come up with a set of codes that encode less
frequently occurring symbols with more bits while encoding more frequently occurring
symbols with less bits.

Take, for example, a `char`. This has a size of 1 byte or 8 bits, but if you only
have 2 different `char`s in your file, then in reality, we only need 1 bit to differentiate
between the two, for example (spacing for display purposes):

<p>
A A B B A B $\rightarrow$ 01000001 01000001 01000010 01000010 01000001 01000010
<br />
compared to
<br />
A A B B A B $\rightarrow$ 0 0 1 1 0 1
</p>

However, it is not as simple as having the patterns `0`, `1`, `01`, `10`, etc. because
when decoding, you would come across ambiguities. Therefore, the encoding must be
a **prefix code** so that none of the codes are prefixes of the other - `0` and
`01` could not both be in the set because `0` is a prefix of `01`.

It just so happens that this can be simply done with a binary tree - we treat the
edges as a way to represent the code and as long as we leave (haha) the symbols at
the leaves, we guarantee that we do not have the prefix conflict. For example:

<script type="text/tikz">
  \begin{tikzpicture}[nodes={draw, circle, minimum size=0.75cm}, ->]
    \node{}
      child{ node{A} edge from parent node[left, draw=none] {0} }
      child{ node {}
        child{ node{B} edge from parent node[left, draw=none] {0} }
        child{ node{C} edge from parent node[right, draw=none] {1} }
      edge from parent node[right, draw=none] {1} };
   \end{tikzpicture}
</script>

We can interpret the tree as a prefix code by starting at the root and using the
bits along the path to each leaf as the codeword for the symbol at the leaf i.e.

| Symbol | Codeword |
| ------ | -------- |
| A      | 0        |
| B      | 10       |
| C      | 11       |

### Huffman coding - an example

Consider compressing the string `AAAACABBDDECCDD`.

To construct this tree for this example, we start with only the leaves (trees of
size 1) - we then combine the two least trees with the least occurring symbols into
another tree and keep doing so until we end up with one tree. Here is a rough
code example.

```python
from collections import namedtuple

Node = namedtuple('Node', ['left', 'right', 'value', 'occurrences'])


def make_huff_tree(symbols):
    if len(symbols) == 1:
        return symbols.pop()

    left = symbols.pop(0)
    right = symbols.pop(0)
    # this is an inner node so there is no value
    symbols.append(
        Node(left, right, None, left.occurrences + right.occurrences))
    # inefficient, but basically I want to guarantee that the first two things
    # in the list are the ones with the least occurrences - this is typically
    # done with a priority queue
    symbols.sort(key=lambda s: s.occurrences)
    return make_huff_tree(symbols)

data = "AAAACABBDDECCDD"
# "fun" way to get char counts
symbols = list(
    map(lambda a: Node(None, None, a[0], len(list(a[1]))), groupby(sorted(data))))
symbols.sort(key=lambda s: s.occurrences)

print(make_huff_tree(symbols))
```

This gives us the following output:

<!-- markdownlint-disable line-length -->
```python
Node(left=Node(left=Node(left=None, right=None, value='C', occurrences=3), right=Node(left=Node(left=None, right=None, value='E', occurrences=1), right=Node(left=None, right=None, value='B', occurrences=2), value=None, occurrences=3), value=None, occurrences=6), right=Node(left=Node(left=None, right=None, value='D', occurrences=4), right=Node(left=None, right=None, value='A', occurrences=5), value=None, occurrences=9), value=None, occurrences=15))
```
<!-- markdownlint-enable line-length -->

Or the following tree (I'm keeping track of the occurences here also, just for reference)

<script type="text/tikz">
  \begin{tikzpicture}[nodes={draw, circle, minimum size=0.75cm}, ->,
    level 1/.style={sibling distance=30mm},
    level 2/.style={sibling distance=15mm}]
    \node{15}
      child { node{6}
        child{ node{C:3} edge from parent node[left, draw=none] {0}
          edge from parent node[left, draw=none] {0}
        }
        child{ node {3}
          child{ node{E:1} edge from parent node[left, draw=none] {0} }
          child{ node{B:2} edge from parent node[right, draw=none] {1} }
          edge from parent node[right, draw=none] {1}
        }
        edge from parent node[left, draw=none] {0}
      }
      child { node {9}
        child { node {D:4} edge from parent node[left, draw=none] {0} }
        child { node {A:5} edge from parent node[right, draw=none] {1} }
        edge from parent node[right, draw=none] {1}
      };
   \end{tikzpicture}
</script>

And encoding

| Symbol | Codeword |
| ------ | -------- |
| A      | 11       |
| B      | 011      |
| C      | 00       |
| D      | 10       |
| E      | 010      |

So here we see that instead of using the usual 8 bits per character, we were able
to compress it down to using 2 or 3 bits per character. So now we know how to compress
data using this algorithm. Now, let's to look at how to store it in a file using
the current example.

As a quick aside, this is the binary of the original file containing $AAAACABBDDECCDD$

```bash
0100000101000001010000010100000101000011010000010100001001000010010001000100010001000101010000110100001101000100010001000
```

When storing the codebook (translation), the one efficient way to store it encodes
a preorder traversal of the tree through bits; imagine 1 is an inner node and 0 is
a leaf node. When we reach a leaf node, we append the symbol. Here's another rough
block of code that builds off the previous one (we'll use a list of strings for
simplicity's sake):

```python
tree = make_huff_tree(symbols)

def encode_huff_tree(tree):
    # a leaf node
    if not (tree.left or tree.right):
        return ['0', format(ord(tree.value), 'b').zfill(8)]
    return ['1'] + encode_huff_tree(tree.left) + encode_huff_tree(tree.right)

print(encode_huff_tree(tree))
```

Which outputs

<!-- markdownlint-disable line-length -->
```python
['1', '1', '0', '01000011', '1', '0', '01000101', '0', '01000010', '1', '0', '01000100', '0', '01000001']
```
<!-- markdownlint-enable line-length -->

We can see that the symbols still take up their usual 8 bits. And the decoding method:

<!-- markdownlint-disable line-length -->
```python
enc_tree = encode_huff_tree(tree)
RebuiltNode = namedtuple('RebuiltNode', ['left', 'right', 'value'])

def decode_huff_tree(encoded):
    node = encoded.pop(0)
    if node == '1':
        # an inner node
        return RebuiltNode(decode_huff_tree(encoded), decode_huff_tree(encoded), None)
    else:
        # a leaf node
        r = RebuiltNode(None, None, chr(int(''.join(encoded[0:8]), 2)))
        del encoded[0:8]
        return r

print(decode_huff_tree(list(''.join(encode_huff_tree(tree)))))
```
<!-- markdownlint-enable line-length -->

Which outputs

<!-- markdownlint-disable line-length -->
```python
RebuiltNode(left=RebuiltNode(left=RebuiltNode(left=None, right=None, value='C'), right=RebuiltNode(left=RebuiltNode(left=None, right=None, value='E'), right=RebuiltNode(left=None, right=None, value='B'), value=None), value=None), right=RebuiltNode(left=RebuiltNode(left=None, right=None, value='D'), right=RebuiltNode(left=None, right=None, value='A'), value=None), value=None)
```
<!-- markdownlint-enable line-length -->

Hey, it's literally the same tree as the one we made all the way back except without
the occurrences data, and we conveniently don't need to give the length of the encoded
tree data because it knows when to terminate, e.g.

```python
enc_tree = encode_huff_tree(tree)
enc_tree = enc_tree + ['oh no!', 'is the data starting?']

# ...

print(decode_huff_tree(list(''.join(enc_tree))) # it still works!
```

And the leftovers of the list are actual the encoded data. There is only one last
thing to do, which is to prepend the encoded tree to the actual data and see how
much space we've saved. Let's look at the individual components before combining
them.

<!-- markdownlint-disable line-length -->
```bash
# encoded tree
# ['1', '1', '0', '01000011', '1', '0', '01000101', '0', '01000010', '1', '0', '01000100', '0', '01000001']
110 01000011 1 0 01000101 0 01000010 1 0 01000100 0 01000001
# actual data
1111111100110110111010010000010100
# final (82)
1100100001110010001010010000101001000100001000001111111110011011011101001000001010
# original (120)
0100000101000001010000010100000101000011010000010100001001000010010001000100010001000101010000110100001101000100010001000
```
<!-- markdownlint-enable line-length -->

Wow, a whopping 38 bits saved! Naturally, compression is more useful when dealing
with larger data. Want to see the full example run end to end? The code can be found
[here](https://github.com/rei2hu/random_stuff/blob/master/python/huffman_example.py)

Whew! That was explanation longer that I initially anticipated - looks like I'll
be splitting this into two parts where part 2 will be all my rambling thoughts.
Honestly, the actual idea for this post popped in my head at like 3 am.
