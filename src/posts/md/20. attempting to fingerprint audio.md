# attempting to fingerprint audio

So I've been spending some time working on identifying audio from smaller clips
and I've finally gotten something that kind of works. The below is a short recap
of what I've tried going through. I believe I used [Otome Dissection Remix](https://www.youtube.com/watch?v=EDjYDfRunUk)
as a previous example so I will use that here, along with the original 2 clips/songs
that made me start this project which are the following:

1. [Dear Doppelganger](https://www.youtube.com/watch?v=grdy6rLbQ-c)
2. This video <video controls src="/blobs/20/stickbugged.mp4" />

## analyzing the frequencies

I spent a lot of time in [Audacity](https://www.audacityteam.org/) looking at the
spectrogram view for songs. Here is an example:

<img src="/blobs/20/otome-dissection-spectrogram-0-10.png" />

This is the spectrograph for the first 10 seconds of Otome Dissection Remix and
you can (more or less) clearly see how it corresponds to the initial (piano?) notes.
Of course, it gets a lot more complex past the 7 second mark.

However, seeing this, it looks like one approach might just be to find how well
one audio sample's graph can be fit on top of another one.

## cross correlation approach

I find the explanation on Wikipedia for how a cross correlation works to be very
understandable.

> As an example, consider two real valued functions $f$ and $g$ differing only by
> an unknown shift along the x-axis. One can use the cross-correlation to find
> how much $g$ must be shifted along the x-axis to make it identical to $f$. The
> formula essentially slides the $g$ function along the x-axis, calculating the
> integral of their product at each position. When the functions match, the value
> of $(f \star g)$ is maximized.

This basically means if one audio clip starts earlier or later than the other,
the cross correlation will be able to determine what the delay is.

Before checking out this approach, I already knew it most likely would not be the
correct approach. The issues that could make this not work that I came up with were:

1. Audio samples need to be more or less perfect clones of each other
2. Only good for checking if one audio sample matches another audio sample when
   what I want is to find the closest matching audio sample from several

If we look at a comparison between the remix and a cover of the remix, we can see
they line up quite similarly.

![spectrogram-comparison-otome](/blobs/20/giga-otome-dissection-remix-spectrogram-0-12.png)

However, I did mark one small spot on the bottom half that differs, and eventually,
due to different vocalists, there are a lot more differences. If I was to try this
with the two audio clips I am looking to compare:

![spectrogram-comparison-doppel](/blobs/20/doppel-stick-56-58.png)

There are some glaring issues. I've actually lined the samples up so they do look
similar, but the entire missing chunk (i.e. comparing a 30 second clip to a 5 minute
clip) makes cross correlation not work out well at all. Furthermore, we see that
there is a lot more white in the top sample. This just means that those frequencies
are played at higher amplifications (the clip does seem a lot more muted compared
to the youtube video). As such, I had to think of some solutions to overcome this.

## dealing with different clip sizes

This one came almost immediately and, while I think it was a good idea, still requires
manual tinkering here and there.

I ended up cutting clips down to their own mini-clips to compare each other with.
Specifically for testing I went with 0.5 seconds like so:

![spectrogram-intervals](/blobs/20/doppel-stick-intervals.png)

Changing the length of the mini-clips does have both costs and benefits and this
is where manual adjustment needs to come into play. Longer intervals are faster to
compare because there are less to compare against, but may also sacrifice accuracy.
You could say before I went with the mini-clip idea I was using mini-clips with
lengths as long as the original clip. If the intervals are too short, then you start
getting too many false positives - this 0.001s of a song sounds like the same as
a ton of other songs.

## dealing with (im)perfect pitch

Because the amplification of the tones played at frequencies did not match up, the
resulting data that was pulled from the audio clips do not exactly match up. So
I ended up borrowing an idea from something I heard of long ago.

I believe there is an audio identifying service where all you input is whether or
not the next "note" is a higher frequency, lower frequency, or the same frequency.
As such I applied this to my idea also. Within each mini-clip I checked the frequency
at fixed intervals and tracked whether it went up, down, or was similar.

I'm not too sure this was a good approach because it tosses away a lot of data.
I think there are definitely some ways to refine this.

## conclusion

So in all, I was able to (kind of) fingerprint audio clips by doing the following:

1. Cutting down the clips themselves to their own 0.5 second clips
2. Dealing with change in frequency rather than actual frequency

In short, we could run a database with numerous audio clips where each audio clip
is split into its own "mini-clips" and those "mini-clips" are fingerprinted by checking
their change in frequency over time. These smaller finger prints are what make up
the fingerprint of the actual audio clip.

## a bonus idea

One problem with directly dealing with frequencies is that there's a ton of extra
"noise" that contributes to it. I have thought about trying to extract the melody
of songs so I would work with "cleaner" data and I am aware of several papers regarding
the topic. I think inserting that as a step before breaking up into miniclips could
definitely improve the accuracy of matching audio clips.
