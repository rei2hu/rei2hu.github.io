# don't take high level abstractions for granted

A couple months ago, I was tasked with fixing a deadly performance issue which was
in the way of completing one of our larger projects for a client.

I'm not going to get into the technologies behind the project (which I am highly
contentious about), but the tldr is that a query generated by the orm was
extremely inefficient.

## figuring out what the hell is going wrong

Fortunately, we had profiling set up so what I was able to find that this was caused
due to an sql query which held up the page from loading (page load depending on query
is also something I'm questionable about, since I would prefer if the data came
in asynchronously)

It boiled down to a several queries formatted like this:

```sql
SELECT
    tasks1.`id`
FROM
    (
        tasks tasks1
        INNER JOIN companies companies1 ON tasks1.`company` = companies1.`id`
    )
WHERE
    tasks1.`type` = 'task_type_1'
    AND (
        tasks1.`company` IN (
            'company1',
            'company2',
            'company3',
            -- literally a billion more ids
        )
    )
    AND companies1.`company_type` = 'company_type_1'
ORDER BY
    tasks1.`tag`
LIMIT
    0, 20
```

### what's wrong

Well actually before I looked at the problem, I jokingly said all we had to do was
throw another index on the table and everything would most likely be fine. When I
looked at this specific query, I noticed that the hangup is most likely in having
to search for the tasks by their company column, which didn't have an index on it.

As such, I thought that an index including company on the task table would help,
so I made a new composite index on the table and tried running it again. It was
still slow - in fact, it was ran under roughly the same amount of time. Why?

Then I tried running EXPLAIN.

<!-- markdownlint-disable line-length -->
```sh
mysql> EXPLAIN (the huge query)
+----+-------------+----------+--------+-------------------+---------+---------+------------------------+------------+-------------------------------------------------------+
| id | select_type | table    | type   | possible_keys     | key     | key_len | ref                    | rows       | Extra                                                 |
+----+-------------+----------+--------+-------------------+---------+---------+------------------------+------------+-------------------------------------------------------+
|  1 | SIMPLE      | tasks1   | ref    | (several indexes) | type    | 243     | const                  | (>500,000) | Using index condition; Using where; Using filesort    |
|  1 | SIMPLE      | company1 | eq_ref | PRIMAY            | PRIMARY | 96      | service.tasks1.company | 1          | Using where                                           |
+----+-------------+----------+--------+-------------------+---------+---------+------------------------+------------+-------------------------------------------------------+
```
<!-- markdownlint-enable line-length -->

As we can see, it was not taking advantage of my index at all - for some reason,
mysql just decides it's better to not use the index I created. So then I tried
forcing the index to be used and got better results.

```sh
normal query - 2862.682 ms
forced index - 1514.601 ms
```

A 2x speedup with that index - which was good, but actually not good enough as the
page still took almost an entire minute to load with a cold cache. The kicker is
that there are several of these kinds of queries so overall it still added up.

### the orm

This is when I started looking at the code that generated the query. It was something
like this:

```js
// simplified version
private static getType2and3Companies() {
    const companies = Companies()
        .where('type', 'company_type_2')
            .or('type', 'company_type_3');
    return [...companies].map(company => company.id);
}

private static getViewableCompanies(self) {
    const companies = UserToCompanies()
        .where('user', self.user)
        .where('company.type', 'company_type_1');
    return [...companies].map(company => company.id);
}

public static getViewableRows(table) {
    const allViewableCompanies = [
        ...this.getType2And3Companies(),
        ...this.getViewableCompanies(),
    ];
    switch(table) {
        case COMPANY:
            return Companies()
                .where('company.type', 'company_type_1');
        case EQUIPMENT:
            return Equipment()
                .where('company', 'in', allViewableCompanies)
                .and('company.type', 'company_type_1')
                .or('type', 'equipment_type_1');
        case TASKS:
            return Tasks()
                .where('company', 'in', allViewableCompanies)
                .and('company.type', 'company_type_1');
        default:
            throw InvalidTableError();
    }
}
```

So the first thought that might cross your mind would be why so many queries when
you could do it in one query and some joins? This was just a limitation of the orm
being used.

### back to sql

Now, there are a few things needed to understand why the above query was most likely
extremely slow. According to the [mysql documentation](https://dev.mysql.com/doc/refman/8.0/en/comparison-operators.html#operator_in)

> If no type conversion is needed for the values in the IN() list, they are all non-JSON
> constants of the same type, and expr can be compared to each of them as a value
> of the same type (possibly after type conversion), an optimization takes place.
> The values the list are sorted and the search for expr is done using a binary
> search, which makes the IN() operation very quick.

For our data, we have a lot of tasks and it according to explain there is basically
a sequential search. However, for each task, we would have to perform that binary
search for the company on the task, which makes it slow i.e.

<p>
Let $m$ be the number of tasks <br />
Let $n$ be the number of companies in the array <br />
The runtime of the query is something like
$$
\begin{align}
c_{1} m * c_{2} \log{n}
\end{align}
$$
Where $c_{1}$ and $c_{2}$ are some constants
</p>

Which is actually not too bad in my opinion. We can see that the query itself is
bound by the number of tasks or the number of companies in the array, so I set out
to cut down one of those. Just as an fyi, tasks are records that would be entered
by the client so there's no way to cut that down - which leaves just cutting down
the companies.

### the fix

Now let's jump back to the code that is generating this query. The important thing
to notice here is that at the end of the builder, in the method `getViewableRows`,
we always filter down the companies to `company_type_1`. It turns out that an
overwheling majority of companies in the `allViewableCompanies` array were of type
2 and 3 but not 1. When I factored that condition out and into a separate method,
the array greatly reduced in length and the page was back to loading in a couple
of seconds.

## takeaways

1. High level abstractions are nice, but each one will have its own strengths and
   weaknesses. The orm has actually simplified the project a lot and we didn't have
   to write any sql queries manually, but it ended up hiding a performance issue.
   Furthermore, in some cases you just won't have the ability to tweak the internals
   and you will have to end up heading in another direction.

2. Don't write code that just works. Yes, the previous code worked, but was extremely
   inefficent. This was actually not revealed during testing because the test engineers
   did not work with such a large set of data when testing (again, maybe a cause
   of the questionable technologies at play). However, a look over the code might
   have hinted towards this problem.

3. Sometimes an efficiency problem can be fixed by just understanding the business
   logic and tweaking the internals. You might not need a specialist to look at stuff
   for you. I'm not really big on sql so fortunately there was a fix I could apply
   which didn't require much knowledge on that.

   Admittedly, I was hoping to use this happening as an argument against the used
   technologies - everyone knows how much I dislike them since I've been quite
   tongue-in-cheek about them - but it was fixed so I was unable to complain as much
   as I wanted to.

## closing

I'd like to point out that I had noticed the step to improve efficiency and mentioned
it as a comment during a code review but it was passed over. Well, I didn't mind
because I did not imagine this kind of problem actually popping up so I thought
it was more of a micro-optimization if anything and wouldn't matter in the long run.

It's also important to note that at the end of the day, nothing truly changed aside
from some internal calculations. If that array does end up getting extremely large
and the performance relapses, I hope I am far away from the project at this time
because the technologies behind it are actually trash.

And that's how I ~~saved~~ contributed to a multi-million dollar contract ~~from
utter doom~~. Actually there's a story behind how this contract was obtained and
some other things about why taking any longer on the deliverance might have been
extremely bad, but I don't think I'm at liberty to give any details on that.
