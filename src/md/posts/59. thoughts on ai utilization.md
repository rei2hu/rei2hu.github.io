# thoughts on ai utilization

With the recent release of Stable Diffusion, I've been browsing a lot of ai generated
materials. It is all quite impressive, but not-uncommonly, I would come across something
that was slightly off. So I decided to write a little bit about considerations that
should be taken when incorporating ai in things.

First things first, I am by no means an expert nor even relatively proficient -
I took one ML course in uni which covered more basic approaches (decision trees,
GA, etc) but then only touched upon more complex ones (RNN), and also took a data
science course of which the main thing I remember was using linear regression models
with various data sets.

The main issue I have with the use of ai is that, when "scaling" it (applying it
to a ton of cases), not many people seem to care about double checking the output.
This is fine, and sometimes funny, in not-so-serious situations like generated artwork,
but when it comes to things like bans from services or something that can result
in negative consequences, the affected usually have no recourse.

Even if the utilizers notice the issue, how then would they improve the models to
avoid such false positives? By re-training it I'd guess. And how would they know
that it worked? If you're someone like me, an average software developer using
these things. How the hell does it even work? If I come across a false positive,
is there a way for me to go in the code and isolate the specific lines that cause
the issue? It's very unlikely - to me, most ai is so complex they're basically black
boxes.

I believe the basic idea is to split your data into training/verification
sets where one part is used for training while the model is verified against the
other. But again, the issue is not with the cases in the training itself but rather
situations that occurr outside the ai's trained scenarios. You constantly have to
keep updating the models if you cared, and you probably have to spend a ton more
effort to get those last few points of accuracy.

Well, I guess from the business perspective ignoring the false positives is worth
it.

And then there is the use of ai to solve problems which have definitive answers;
things like random trivia or even writing code! In these cases, generating a
correct answer is important. It's kind of crazy to see how many people think
this is a good thing - perhaps they believe in the accuracy of ai - without
considering the impact an inaccurate or incorrect answer could have. Again, I
think it boils down the need for human verification of the outputs, but if you
do any kind of collaborative software engineering, it is sometimes *harder* to
audit existing code that someone else has written than it is to write it
yourself. It is also possible that verification of answers is simpler than
generating them.

In short, the complex black box types of ai that are gaining popularity for use
in today's systems are more suited for subjective tasks where new perspectives
are welcome, while its usage for objective tasks should be limited. In both
cases, it should be pair with human oversight to avoid issues.
