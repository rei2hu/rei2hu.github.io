# lack of attention to detail

My work has recently been pushing generative AI, and some of my co-workers
appear to have been using it. I also have been using it, and my feelings have
been mixed. On the other hand, I have strong negative feelings when it comes to
some co-workers' use of it.

Recently, I've been dealing with some really poor pull requests. I mean, the
quality is typically poor overall (as in not many people will fully fill out the
template and leave the placeholder text in like "tests: fill in test info
here"), but now it's gotten even worse. Things I looked at had glaring issues:

1. Getting the name of things wrong that you got right elsewhere. In this case,
   there was a variable `x = "abc-app"` which was in the code, and in one place
   `x` was used, and in another place `app-abc` was used. The reason this gets
   me is because I'm not sure how this kind of issue occurs; how can you make a
   variable then forget to use it (there were no barriers to usage, this is the
   same block of code) and then mistype the string itself?
2. The obvious incorrect usage of arguments. Here, the dev got a token
   `AUTH_TOKEN = ...`, then provided the token as the version argument to a
   script `--version AUTH_TOKEN`. Again, this kind of mistake blows my mind
   because I don't know how it can be made and not caught before being submitted
   for (final) code review (not like a draft review, which is optional).
3. A wordy description that described a bunch of stuff that was _not_ done as
   part of the pull request

    > ## ðŸš§ Summary
    >
    > - deleted file.json
    > - cleaned up xyz.rs
    > - ...
    >
    > ## ðŸ”§ Technical Details
    >
    > - Refactored timers in xyz.rs to handle any number of requests coming in
    > - ...

    I can't really replicate the style without copy/pasting it, and I don't want
    to do that, but what I will say is that was some LLM chatter if I ever read
    any.

    60,000 line pull request by the way.

4. Calling made up APIs. Typical hallucination slop.
5. Working code that does things the wrong way. In a test, there was some
   mocking code to override the app configuration so that a specific url would
   be used. But really you could just set it in the test configuration which is
   what's used in the test environment.

And that was all in one day. I think a lot of career software engineers are just
not actually detail oriented enough for the job. And I hate this - I think I've
said it before.

Anyways, let's look at what I got out of my generative AI usage (Claude
Code/Cursor, by the way).

One issue was where I was writing some new custom serialization logic and, for
some reason, it was not applying within a test. I had hit a barrier for a while,
and I asked used the AI chat for some help. I described the problem, and it
generated test cases for me to confirm that things were not working. I then had
to explain the logic because this spanned across several repositories (because
of libraries). It kept writing tests and saying my change in the library itself
must be wrong because it's not working, but I was very sure that it was. It
spent most of its time writing tests and saying I was wrong, but then saying I
was right, and it could never figure out the problem. Then problem was that I
forgot to use snake case in the test data so serialization was not applying to
camel cased properties... Whoops!

Another time is when I had to do a simple field extraction on a lucene
(Elasticsearch syntax) query. It went the regular expression route with several
very complex regular expressions, and I pointed out that quotes might make it
fall over and it should try something else. Instead it kept trying to patch up
the quote issue by introducing _even more_ regular expressions (which I am NOT
reviewing nor submitting). The answer is `QueryBuilder` by the way.

Another was me trying to stand up some infrastructure with Terraform. It
generated all the config, but parts of it were not valid. It wasn't too bad once
I went over and fixed it though. That was a good time save.

Overall, I still have the same negative opinion on LLMs, and I think that the
tools output is only as good as its user. Can you get something good? Maybe.
"100%" good? Eh, probably not, but the problem is figuring out what that missing
part is and how important it is. If it's an unknown unknown, so to speak, then
you won't catch it, and it may turn into a big issue later down the line. Like
your database being unsecured. Or it could be something simple like some text
isn't perfectly aligned. Will I take that risk? No, but I think many other
people will. To their work's detriment.

On a semi-related note and tangent, I have friends who can't code but do rely on
LLMs to complete hobby-level coding related tasks, and I find that very
impressive. The problem is that when they run into a problem, the only way they
can get out is through more prompts, and that doesn't always work (though I'll
usually help if the work isn't too messy, yet). We have a running joke - "just
one more prompt" - which I find hilarious.

I will also note that a lot of defenses for prompting revolve around not
prompting "correctly", which is... kind of weird to me. I don't doubt that
wording your prompt differently or formatting in some specific way improves
results, but the approach seems quite arcane, in my opinion.
