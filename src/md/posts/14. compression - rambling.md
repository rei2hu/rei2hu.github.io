# compression - rambling

Warning.

## is this a valid idea

So my understanding is that data is more easily compressed the less random it
is. However, there is overhead for storing the decompression data, in the
Huffman case, the tree (well also the decompression script counts towards
overhead, too I guess).

However, it stands to reason that it's possible to make the data less random by
splitting it up.

## let's try it

Consider the string `5d2k6c3q1dw5x2y9sy3z4:vnhx6r96t8` - it looks quite random
doesn't it? And compressing it using Huffman coding ends up giving values like
this:

```python
Unencoded Length: 256
Unencoded: 0011010101100100001100100110101100110110011000110011001101110001001100010110010001110111001101010111100000110010011110010011100101110011011110010011001101111010001101000011101001110110011011100110100001111000001101100111001000111001001101100111010000111000
Encoded Length: 371
Encoded: 11110011110100001100101000110011000110101110001110010011001001001111000001111001111100011000100011010010001110000001110101100110001100110100010011010110011011101110011100010011100101001110011001110100110011101100011101110001101100011010100011011011111010000101100010000010111101001101100001011101001101001110010000010001100111110010111101010110111111001010011111101110010
Data: 5d2k6c3q1dw5x2y9sy3z4:vnhx6r96t8
Decoded: 5d2k6c3q1dw5x2y9sy3z4:vnhx6r96t8
```

We can see that trying to compress it has actually increased the size from 256
to 371 bytes. But let's try modifying our script from the last part to chunk the
data to various sizes and get the chunk size that gives us the smallest
compressed data size; something quick like this:

```python
best_chunk_size = len(data)
best_encoded_size = 371
for i in range(2, len(data)):
    total_encoded_size = 0
    chunks = [data[x:x + i] for x in range(0, len(data), i)]
    for chunk in chunks
        # normal processing logic
        total_encoded_size = total_encoded_size + len(encoded)
    if total_encoded_size < best_encoded_size:
        best_chunk_size = i
        best_encoded_size = total_encoded_size
```

It turns out that breaking it out into chunks of 2 is actually a lot more
efficient.

```python
Unencoded Length: 16
Unencoded: 0011010101100100
Encoded Length: 21
Encoded: 100011010100110010001
Data: 5d2k6c3q1dw5x2y9sy3z4:vnhx6r96t8
Decoded: 5d

Unencoded Length: 16
Unencoded: 0011001001101011
Encoded Length: 21
Encoded: 100011001000110101101
Data: 5d2k6c3q1dw5x2y9sy3z4:vnhx6r96t8
Decoded: 2k

Unencoded Length: 16
Unencoded: 0011011001100011
Encoded Length: 21
Encoded: 100011011000110001101
Data: 5d2k6c3q1dw5x2y9sy3z4:vnhx6r96t8
Decoded: 6c

Unencoded Length: 16
Unencoded: 0011001101110001
Encoded Length: 21
Encoded: 100011001100111000101
Data: 5d2k6c3q1dw5x2y9sy3z4:vnhx6r96t8
Decoded: 3q

Unencoded Length: 16
Unencoded: 0011000101100100
Encoded Length: 21
Encoded: 100011000100110010001
Data: 5d2k6c3q1dw5x2y9sy3z4:vnhx6r96t8
Decoded: 1d

Unencoded Length: 16
Unencoded: 0111011100110101
Encoded Length: 21
Encoded: 100011010100111011110
Data: 5d2k6c3q1dw5x2y9sy3z4:vnhx6r96t8
Decoded: w5

Unencoded Length: 16
Unencoded: 0111100000110010
Encoded Length: 21
Encoded: 100011001000111100010
Data: 5d2k6c3q1dw5x2y9sy3z4:vnhx6r96t8
Decoded: x2

Unencoded Length: 16
Unencoded: 0111100100111001
Encoded Length: 21
Encoded: 100011100100111100110
Data: 5d2k6c3q1dw5x2y9sy3z4:vnhx6r96t8
Decoded: y9

Unencoded Length: 16
Unencoded: 0111001101111001
Encoded Length: 21
Encoded: 100111001100111100101
Data: 5d2k6c3q1dw5x2y9sy3z4:vnhx6r96t8
Decoded: sy

Unencoded Length: 16
Unencoded: 0011001101111010
Encoded Length: 21
Encoded: 100011001100111101001
Data: 5d2k6c3q1dw5x2y9sy3z4:vnhx6r96t8
Decoded: 3z

Unencoded Length: 16
Unencoded: 0011010000111010
Encoded Length: 21
Encoded: 100011010000011101001
Data: 5d2k6c3q1dw5x2y9sy3z4:vnhx6r96t8
Decoded: 4:

Unencoded Length: 16
Unencoded: 0111011001101110
Encoded Length: 21
Encoded: 100110111000111011010
Data: 5d2k6c3q1dw5x2y9sy3z4:vnhx6r96t8
Decoded: vn

Unencoded Length: 16
Unencoded: 0110100001111000
Encoded Length: 21
Encoded: 100110100000111100001
Data: 5d2k6c3q1dw5x2y9sy3z4:vnhx6r96t8
Decoded: hx

Unencoded Length: 16
Unencoded: 0011011001110010
Encoded Length: 21
Encoded: 100011011000111001001
Data: 5d2k6c3q1dw5x2y9sy3z4:vnhx6r96t8
Decoded: 6r

Unencoded Length: 16
Unencoded: 0011100100110110
Encoded Length: 21
Encoded: 100011011000011100110
Data: 5d2k6c3q1dw5x2y9sy3z4:vnhx6r96t8
Decoded: 96

Unencoded Length: 16
Unencoded: 0111010000111000
Encoded Length: 21
Encoded: 100011100000111010010
Data: 5d2k6c3q1dw5x2y9sy3z4:vnhx6r96t8
Decoded: t8
```

We were able to cut the data down to 336 bits, compared to the original 371, but
it was still worse than the original unencoded length of 256. Also this is when
I realized there seemed to be a missing case for encoding when there was only 1
symbol. But that's not important considering what I'm about to explain.

First, let's determine the size of the encoding of the binary tree relative to
the number of symbols. We know that there will be $1:1$ relationship between the
number of nodes and the number of 1 and 0 bits in the encoding. We also know
that there will be $8*len(symbols)$ of bits to encode the symbol data for the
tree (assuming we're using chars which are 1 byte long). We also know that we
will always have a full binary tree because we build from the leaves up, so
let's find a relationship between number of nodes in a full binary tree and
number of leaves. Examples are good to look at.

$$
\begin{tikzpicture}[nodes={draw, circle, minimum size=0.75cm}, ->]
  \node{}
    child{ node{} }
    child{ node {} };
\end{tikzpicture}
$$

<br />

$$
\begin{tikzpicture}[nodes={draw, circle, minimum size=0.75cm}, ->]
  \node{}
    child{ node{} }
    child{ node {}
      child{ node{} }
      child{ node{} } };
\end{tikzpicture}
$$

<br />

$$
\begin{tikzpicture}[nodes={draw, circle, minimum size=0.75cm}, ->,
  level 1/.style={sibling distance=30mm},
  level 2/.style={sibling distance=15mm}]
  \node{}
    child { node {}
      child { node {} }
      child { node {} }
    }
    child { node{}
      child{ node{} }
      child{ node {} }
    };
\end{tikzpicture}
$$

<br />

$$
\begin{tikzpicture}[nodes={draw, circle, minimum size=0.75cm}, ->,
  level 1/.style={sibling distance=30mm},
  level 2/.style={sibling distance=15mm}]
  \node{}
    child { node {}
      child { node {} }
      child { node {} }
    }
    child { node{}
      child{ node{} }
      child{ node {}
        child{ node{} }
        child{ node{} }
      }
    };
\end{tikzpicture}
$$

<br />

| Leaves | total nodes |
| ------ | ----------- |
| 1      | 1           |
| 2      | 3           |
| 3      | 5           |
| 4      | 7           |
| 5      | 9           |

And we should be able to see that we the pattern is $n = 2l - 1$. And remember,
the number of leaves is equal to the number of symbols.

Let $s$ be the number of distinct symbols in the data

<!--markdownlint-disable no-space-in-emphasis-->

$$
\begin{align}
8 * len(symbols) + 2 * len(symbols) - 1 = 10 * len(symbols) - 1
\end{align}
$$

<!--markdownlint-enable no-space-in-emphasis-->

So it takes $10 * s - 1$ bits to encode the binary tree (double check that...).
Now we need to think about how the data is actually encoded.

There is one obvious point where the encoding is no longer worth it, and that is
when the encoding for a node ends up being _at least_ 8 bits long - or in other
words the depth the leaf nodes of the tree are all at least 8.

This definitely requires a lot more thought and analysis than my ramble is
calling for, but the idea is that there should be some ratio between the
occurrence of shorter encoded symbols and longer encoded symbols that, once
surpassed, can show Huffman coding to be inefficient. The idea is that a tree
like the following:

$$
\begin{tikzpicture}[nodes={draw, circle, minimum size=0.75cm}, ->]
  \node{}
    child { node {A} }
    child { node{}
      child{ node{} }
      child{ node {}
        child{ node{} }
        child{ node{}
          child { node {} }
          child { node {} }
        }
      }
    };
\end{tikzpicture}
$$

But extended far and deep to the right. We can see that $A$ only takes one bit
to encode but the symbols much lower down will - possibly - a lot more than 8
bits to encode, so for Huffman coding to be efficient, there needs to be a
certain amount of more $A$s compared to the ones down there - intuition says the
ratio is also related to the difference in bit encoding lengths.

I think the ramble has gone on for some time, so I'd like to just end with one
example where chunking beats the unencoded while chunking doesn't - when the
data is $ABCABCDEFDEFGHIGHI$.

Just by looking at it, you can already guess that a good chunk size is going to
be 6, and it turns out that is what is best. If we go with encoding the entire
thing, we get

```python
Unencoded Length: 144
Unencoded: 010000010100001001000011010000010100001001000011010001000100010101000110010001000100010101000110010001110100100001001001010001110100100001001001
Encoded Length: 147
Encoded: 111001000011001000100100100010100100011011001000111001001000100100100110010000010010000101110111100011101111000001010011001010011100101110100101110
Data: ABCABCDEFDEFGHIGHI
Decoded: ABCABCDEFDEFGHIGHI
```

3 more bits! But chunking it down to chunks of 6, we get

```python
Unencoded Length: 48
Unencoded: 010000010100001001000011010000010100001001000011
Encoded Length: 39
Encoded: 100100001110010000010010000101011010110
Data: ABCABCDEFDEFGHIGHI
Decoded: ABCABC

Unencoded Length: 48
Unencoded: 010001000100010101000110010001000100010101000110
Encoded Length: 39
Encoded: 100100011010010001000010001011011010110
Data: ABCABCDEFDEFGHIGHI
Decoded: DEFDEF

Unencoded Length: 48
Unencoded: 010001110100100001001001010001110100100001001001
Encoded Length: 39
Encoded: 100100100110010001110010010001011010110
Data: ABCABCDEFDEFGHIGHI
Decoded: GHIGHI
```

117 bits! An improvement of 27 bits!

## further reading

Looking at the
[wikipedia page for Huffman coding](https://en.wikipedia.org/wiki/Huffman_coding),
there are two other algorithms mentioned:
[arithmetic coding](https://en.wikipedia.org/wiki/Arithmetic_coding) and
[asymmetric numeral systems](https://en.wikipedia.org/wiki/Asymmetric_numeral_systems).
Pretty interesting stuff.

Also this [blog](http://fastcompression.blogspot.com/) is very informative of
other related topics.

Ramble over.
