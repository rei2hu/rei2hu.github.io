# brute force compression idea

This randomly popped into my head as I was thinking of compression algorithms;
How efficient would it be to compress data by:

1. Calculating a hash for the original data
2. Removing parts of the data and mark them as missing

I'll talk about it in the context of compressing strings because it's easier for
me to use terminology, but I don't see why it's not applicable to any kind of
data.

The decoding process would be done through brute force, and its run-time would
depend on two things; the character dictionary - how many valid possibilities go
in a single spot - the number of removed characters, and the hash function
itself.

I'm not competent enough to do any kind of analysis on this idea, but the things
to ask yourself would be:

1. What are the chances two pieces of data with the same length and same parts
   in various positions can cause a collision e.g. could AB\_ have two values
   for \_ that hash to `902fbdd2b1df0c4f70b4a5d23525e932`? (that's the md5 for
   ABC). The chance does increase with length of data, so one way to lower the
   risk would be to chunk the data and apply it to each chunk.
2. How do we signify where characters are removed? Replacing them with a "blank"
   won't do much for compression, so ideally we remove various ranges in the
   data and mark them somehow.
3. How can we find "ideal" solutions? We can speed up decoding if we lower the
   amount of unique characters that are removed, which may require including the
   character dictionary in the data or by lowering the amount of overall
   characters removed. On the other hand, we can increase compression by
   removing more characters. Various hashing functions also have benefits
   compared to others for example, in general, a rolling hash may help when
   decoding.

I do think that storage is cheaper than compute, and that the unreliable nature
of this idea is so bad that it's probably not worth it.
