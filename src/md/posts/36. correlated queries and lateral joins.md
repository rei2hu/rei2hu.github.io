# correlated queries and lateral joins

Recently I was able to try my hand at improving a query's performance for loading
some data. For the sake of anonymity, I'll rephrase the problem it was solving;
imagine having a table of users and a table of comments and you want to get the
top users based on the "max" comment - max in quotes because the ordering is one
of the inputs.

Here are some fake tables:

<!-- markdownlint-disable line-length -->
```sql
create table person (id uuid primary key);
create table comment (id uuid primary key, person_id uuid references person(id), created_at date not null, edited_at date);
```
<!-- markdownlint-enable line-length -->

The query being run was used a correlated subquery like so
```sql
select *
from person
join comment
on person.id = (select id
    from comment
    where comment.person_id = person.id
    and created_at > x
    order by created_at
    limit 1)
order by created_at
limit 10;
```

This isn't necessarily ideal because the subquery is executed each for each person,
however for our case it was quite fine because the subquery was able to take advantage
of an index to be fast and 10 fasts was still fast... most of the time.

To rewind a bit, some "persons" happened to have around 8 million "comments", and
because we had so many comments, we ended up partitioning the comment table on the
`created_at` column. This means that Postgres is able to take advantage of this
information and find the "max" of one partition and know its higher than any other
entry in another partition via an `index only scan (backwards)`.

```
-> Index Only Scan Backward using comment_202103_partition_person_id_created_at_idx
     Index Cond: ((person_id = person.id) AND ...)
     ... (actual time=0.002..0.002 rows=0 loops=15000)
```

This query takes around 5-20 seconds to execute for us depending on another few inputs.

The issue comes when we start sorting by another column, like `edited_at`.

```
select *
from person
join comment
on person.id = (select id
    from comment
    where user.person_id = person.id
    and created_at > x
    order by edited_at
    limit 1)
order by edited_at
limit 10;
```

Now, postgres can't take advantage of the partition to find the "max" comment. All
we really had noticed was that the query was timing out, and we had a 30 second
timeout for it. The first thing I did was run it with `explain analyze` and it turns
out it was taking between 8-10 minutes to complete (!).

```
-> Index Scan Backward using ...
     Index Cond: ((person_id = person.id) AND ...)
     Filter: (created_at > x)
     Rows Removed by filter: 50000
     ... (actual time=24.229..24.229 rows=0 loops=15000)
```

Now, it might have been possible to put another index that covers both all the relevant
columns, but I figured it might just be simpler to rewrite the query. And also because
indexes and partitioned tables are kind of a mess I believe and the tooling around
testing indexes are not there - suggestion was to create index in transaction then
analyze then remove. Because I knew the bulk of time for the query was due to the
subquery, I tried rewriting it initially to first find max comments and then join
with the person table.

I had a couple of attempts:

```psql
select *
from person
join (select distinct on(person_id) *
    from comment
    order by person_id, edited_at
    ) as x
on x.person_id = person.id
order by edited_at
limit 10;
```

```psql
select *
from person
join comment
on comment.person_id = person.id
join (select id, max(edited_at)
    from comment
    group by person_id) as x
on comment.id = x.id and comment.edited_at = x.edited_at;
```

These are some rough approximations but ended up having some pretty good performance,
down to 15-20 seconds from 8-10 minutes. Unforunately it didn't really compare well
for the `created_at` cases which went from a few seconds up to 1-2 minutes. Furthermore,
we used jooq and I had trouble getting subquery columns to work nicely in it.

I eventually settled on just replacing the correlated subquery with a lateral join,
thinking that it should be a straight upgrade in all cases because its main effect
would be to prevent unnecessary extra loops.

```psql
select *
from person
cross join lateral (select *
    from comment
    where person.id = comment.person_id
    and created_at > x
    order by edited_at
    limit 1)
order by edited_at
limit 10;
```

Note that I think the following is also possible due to the `limit 1`

```psql
select *
from person, lateral (select *
    from comment
    where person.id = comment.person_id
    and created_at > x
    order by edited_at
    limit 1)
order by edited_at
limit 10;
```

But I went with the first version because it was a lot easier to deal with jooq
that way (and a simpler change means people are more willing to give me a quicker
review). And the nice thing over rewriting the query is that we can also see the
exact change in the plan that causes the speed up.

```
-> Index Scan Backward using ...
     Index Cond: ((person_id = person.id) AND ...)
     Filter: (created_at > x)
     Rows Removed by filter: 50000
     ... (actual time=25.002..25.002 rows=0 loops=1500)
```

1/10th of the loops! And the actual time of the query did go down to somewhere in
the 30-40 second range. According to the analyze at least. But wait, the timeout
is 30 seconds and you're still over 30 seconds. Yes, but this is in our development
environment. Manual tests in prod suggest improvement should change duration from
2 minutes to around 10 seconds.
